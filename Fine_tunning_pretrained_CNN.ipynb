{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine_tunning_pretrained_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49LUc3IomXCz"
      },
      "source": [
        "# Welcome to my Convolutional Neural Network + Random Forest Classification Notebook\n",
        "The aim of this notebook is to use a pretrained Convolutional Neural Network 'VGG-16' in addition to a customized chosen Classifier 'Random Forest'in order to classify digit images.\n",
        "\n",
        "I will be using keras library in this notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOyhTz8rnXmZ"
      },
      "source": [
        "## Packages used in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq0xp4MsvvNw"
      },
      "source": [
        "#!pip install tqdm\n",
        "#!pip install tensorflow-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmgfhbOKnZu3"
      },
      "source": [
        "I will use GPU in order to accelerate training and prediction phases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rO9QhWi6cKDO",
        "outputId": "55431444-b834-449f-9e61-d8ffdf707561"
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58HAcU9unsgE"
      },
      "source": [
        "## Importing the 'MNIST' Dataset\n",
        "\n",
        "The MNIST dataset contains 70 000 images of digits ranging from 0 to 9.<br>\n",
        "The training set wcontains 60 000 images. While the test set contains 10 000 images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q98uLwQUoIVY"
      },
      "source": [
        "Importing and preprocessing the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcD4Vru1h6e2",
        "outputId": "5b3dc247-43ee-4804-bfec-78e2daba4dab"
      },
      "source": [
        "from tensorflow.keras import datasets \n",
        "import numpy as np\n",
        "import cv2\n",
        "SIZE = 100\n",
        "\n",
        "(x_train_raw, y_train_raw), (x_test_raw, y_test_raw) = datasets.mnist.load_data()\n",
        "x_train_raw, y_train_raw, x_test_raw, y_test_raw = x_train_raw, y_train_raw, x_test_raw, y_test_raw\n",
        "x_train = []\n",
        "for img in x_train_raw:\n",
        "  img = cv2.resize(img, (SIZE, SIZE))\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "  x_train.append(img)\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "\n",
        "x_test = []\n",
        "for img in x_test_raw:\n",
        "  img = cv2.resize(img, (SIZE, SIZE))\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "  x_test.append(img)\n",
        "\n",
        "x_test = np.array(x_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDtZ5iUEoREy"
      },
      "source": [
        "## Importing the pretrained 'VGG-16' model\n",
        "The aim of this part is to exploit the capacity of the pretrained model to recognize specific features.\n",
        "\n",
        "I will freeze the layers of this model so that the weights doesn't change during the training phase. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0MSu1O4ld-y",
        "outputId": "fe1d597e-6f14-4910-8eae-a0665838fac6"
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "with tf.device('/device:GPU:0'):\n",
        "  vgg16_mdl = VGG16(weights='imagenet', include_top=False, input_shape=(SIZE, SIZE, 3))\n",
        "  for layer in vgg16_mdl.layers:\n",
        "    layer.trainable = False\n",
        "vgg16_mdl.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 2s 0us/step\n",
            "58900480/58889256 [==============================] - 2s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 100, 100, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 100, 100, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 100, 100, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 50, 50, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 50, 50, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 50, 50, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 25, 25, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 25, 25, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 25, 25, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 25, 25, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 12, 12, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 12, 12, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsHShEAhpc16"
      },
      "source": [
        "## Importing and Training Random Forest classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umUv2SiNoJkz"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "with tf.device('/device:GPU:0'):\n",
        "  classifier_mdl = RandomForestClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKmKkzfquUqi"
      },
      "source": [
        "# Function used to split data into batches\n",
        "def batch(iterable, size=1):\n",
        "    l = len(iterable)\n",
        "    for ndx in range(0, l, size):\n",
        "        yield iterable[ndx:min(ndx + size, l)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVJUqSrGpbPM"
      },
      "source": [
        "Training the Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA5K-4Cwpsxd",
        "outputId": "451f15f9-822e-409b-8ec0-5b611aca1722"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  x_train_batches = list(batch(x_train, 500))\n",
        "  y_train_batches = list(batch(y_train_raw, 500))\n",
        "\n",
        "  for i in tqdm(range(len(x_train_batches))):\n",
        "      base_output = vgg16_mdl.predict(x_train_batches[i])\n",
        "      inter_output = base_output.reshape(base_output.shape[0], -1)\n",
        "      classifier_mdl.fit(inter_output, y_train_batches[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 120/120 [03:52<00:00,  1.94s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz4PCVLdpjH4"
      },
      "source": [
        "## Evaluate the model\n",
        "In this part, we use our stacked model 'VGG-16 + Random Forest Classifier' to predict the results on the testing/validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "840YPPbLVQdk",
        "outputId": "59f2a264-15fa-4c41-c642-e8e7ae3bfbcf"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  x_test_batches = list(batch(x_test, 200))\n",
        "  pred = []\n",
        "  for i in tqdm(range(len(x_test_batches))):\n",
        "      base_output = vgg16_mdl.predict(x_test_batches[i])\n",
        "      inter_output = base_output.reshape(base_output.shape[0], -1)\n",
        "      pred += classifier_mdl.predict(inter_output).tolist()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:28<00:00,  1.75it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fwmbdknnp27K"
      },
      "source": [
        "We can see that just by using the original predefined weights of the VGG-16 CNN model and training the classifier, we reached 90% accuracy on test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEhNTYswadHl",
        "outputId": "1315a5a8-873a-4777-acef-15893cf0a9e9"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print(\"Accuracy = \", metrics.accuracy_score(y_test_raw, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy =  0.8999\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}